{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/skwong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/skwong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/skwong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster = pd.read_csv('nlp-getting-started/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## add features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array(disaster[['id','text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1,\n",
       "        'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'],\n",
       "       [4, 'Forest fire near La Ronge Sask. Canada'],\n",
       "       [5,\n",
       "        \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"],\n",
       "       [6,\n",
       "        '13,000 people receive #wildfires evacuation orders in California '],\n",
       "       [7,\n",
       "        'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes words that are not needed. Lemmaizes words\n",
    "def clean_text(arr):\n",
    "    cleaned_texts = []\n",
    "    for row in arr:\n",
    "        text = \"\".join((char for char in row[1] if char not in string.punctuation))\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', row[1], flags=re.MULTILINE)\n",
    "        text = word_tokenize(text)\n",
    "        text = [lemmatizer.lemmatize(w.lower()) for w in text if not w in stop_words] \n",
    "        cleaned_texts.append(text)\n",
    "        \n",
    "    return np.array(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = clean_text(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster['clean_text'] = text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains sentiment score\n",
    "def sentiment(arr):\n",
    "    scores = []\n",
    "    for row in arr:\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', row[1], flags=re.MULTILINE)\n",
    "        text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', row[1], flags=re.MULTILINE)\n",
    "        scores.append(list(analyser.polarity_scores(text).values()))\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentiment = sentiment(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = [t_s[0] for t_s in text_sentiment]\n",
    "neutral = [t_s[1] for t_s in text_sentiment]\n",
    "negative = [t_s[2] for t_s in text_sentiment]\n",
    "compound = [t_s[3] for t_s in text_sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster['sentiment_score_positive'] = positive\n",
    "disaster['sentiment_score_neutral'] = neutral\n",
    "disaster['sentiment_score_negative'] = negative\n",
    "disaster['sentiment_score_compound'] = compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment_score_positive</th>\n",
       "      <th>sentiment_score_neutral</th>\n",
       "      <th>sentiment_score_negative</th>\n",
       "      <th>sentiment_score_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deed, reason, #, earthquake, may, allah,...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, resident, asked, 'shelter, place, ', not...</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, #, wildfire, evacuat...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, sent, photo, ruby, #, alaska, smok...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \\\n",
       "0       1  [our, deed, reason, #, earthquake, may, allah,...   \n",
       "1       1   [forest, fire, near, la, ronge, sask, ., canada]   \n",
       "2       1  [all, resident, asked, 'shelter, place, ', not...   \n",
       "3       1  [13,000, people, receive, #, wildfire, evacuat...   \n",
       "4       1  [just, got, sent, photo, ruby, #, alaska, smok...   \n",
       "\n",
       "   sentiment_score_positive  sentiment_score_neutral  \\\n",
       "0                     0.000                    0.851   \n",
       "1                     0.286                    0.714   \n",
       "2                     0.095                    0.905   \n",
       "3                     0.000                    1.000   \n",
       "4                     0.000                    1.000   \n",
       "\n",
       "   sentiment_score_negative  sentiment_score_compound  \n",
       "0                     0.149                    0.2732  \n",
       "1                     0.000                   -0.3400  \n",
       "2                     0.000                   -0.2960  \n",
       "3                     0.000                    0.0000  \n",
       "4                     0.000                    0.0000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count\n",
    "disaster['word_count'] = disaster['clean_text'].apply(lambda x: len(x))\n",
    "# hashtag_count\n",
    "disaster['hashtag_count'] = disaster['clean_text'].apply(lambda x: len([c for c in x if c == '#']))\n",
    "#df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "# mention_count\n",
    "disaster['mention_count'] = disaster['clean_text'].apply(lambda x: len([c for c in x if c == '@']))\n",
    "#df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1,\n",
       "        'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'],\n",
       "       [4, 'Forest fire near La Ronge Sask. Canada'],\n",
       "       [5,\n",
       "        \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"],\n",
       "       ...,\n",
       "       [10871,\n",
       "        'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ'],\n",
       "       [10872,\n",
       "        'Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.'],\n",
       "       [10873,\n",
       "        'The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_tfidf(arr):\n",
    "    cleaned_texts = []\n",
    "    for row in arr:\n",
    "        text = \"\".join((char for char in row[1] if char not in string.punctuation))\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', row[1], flags=re.MULTILINE)\n",
    "        text = word_tokenize(text)\n",
    "        text = [lemmatizer.lemmatize(w.lower()) for w in text if not w in stop_words]\n",
    "        text = ' '.join(text)\n",
    "        text = text.replace('# ','#')\n",
    "        cleaned_texts.append(text)\n",
    "        \n",
    "    return np.array(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text_tfidf(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['our deed reason #earthquake may allah forgive u',\n",
       "       'forest fire near la ronge sask . canada',\n",
       "       \"all resident asked 'shelter place ' notified officer . no evacuation shelter place order expected\",\n",
       "       ...,\n",
       "       'm1.94 [ 01:04 utc ] ? 5km s volcano hawaii . http : //t.co/zdtoyd8ebj',\n",
       "       'police investigating e-bike collided car little portugal . e-bike rider suffered serious non-life threatening injury .',\n",
       "       'the latest : more home razed northern california wildfire - abc news http : //t.co/ymy4rskq3d'],\n",
       "      dtype='<U195')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it in tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(min_df = 5, ngram_range = (2,2))\n",
    "features = tfidf.fit_transform(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(features.todense(),columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 1227)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster.reset_index(drop=True, inplace=True)\n",
    "features_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster = pd.concat([disaster,features_df],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 1240)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target', 'clean_text',\n",
       "       'sentiment_score_positive', 'sentiment_score_neutral',\n",
       "       'sentiment_score_negative', 'sentiment_score_compound',\n",
       "       ...\n",
       "       'youtube video', 'z10 full', 'û_ http', 'ûª http', 'ûª israel',\n",
       "       'ûªs first', 'ûªs stock', 'ûªt let', 'ûªve home', 'ûïwhen saw'],\n",
       "      dtype='object', length=1240)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_drop = disaster.drop(['id','keyword','location','text','clean_text'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_list = []\n",
    "for t in cleaned_text:\n",
    "    clean_text_list.extend(t.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = []\n",
    "for t in cleaned_text:\n",
    "    lst = t.split(' ')\n",
    "    clean_list.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our', 'deed', 'reason', '#earthquake', 'may', 'allah', 'forgive', 'u']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(clean_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22444"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6493"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=20, padding_start=True):\n",
    "    # text = clean_list[i]\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in text])\n",
    "    l = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:l] = enc1[:l]\n",
    "    else:\n",
    "        enc[N-l:] = enc1[:l]\n",
    "    return enc, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32),\n",
       " 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(clean_list[0], vocab2index, N=40, padding_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(disaster_drop.drop('target', axis = 1))\n",
    "y = np.array(disaster_drop.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.207, 0.536, 0.257, ..., 0.   , 0.   , 0.   ],\n",
       "       [0.205, 0.795, 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       ...,\n",
       "       [0.155, 0.845, 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.   , 1.   , 0.   , ..., 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Creates RandomizedSearch/ GridSearch CV objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defines Dictionaries for possible values to choose from\n",
    "ada_boost_dict = {'n_estimators': np.linspace(50, 500, num=46, dtype = int), \n",
    "                  'learning_rate':np.linspace(0.05,1,num=20), \n",
    "                  'algorithm' : ['SAMME', 'SAMME.R']}\n",
    "\n",
    "log_dict = {'penalty': ['l2','none'], \n",
    "            'fit_intercept': [True, False],\n",
    "           'C': np.linspace(0,5,21)}\n",
    "\n",
    "rf_dict = {'n_estimators': np.linspace(50, 1000, num=96, dtype = int),\n",
    "          'criterion':['gini','entropy'],\n",
    "          'min_samples_split':np.linspace(5,50,11, dtype = int),\n",
    "          'max_features':['auto','sqrt','log2',None],\n",
    "          'bootstrap':[True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier()\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "log_reg = LogisticRegression(multi_class = 'ovr', max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_model = RandomizedSearchCV(adaboost, ada_boost_dict, 60, \n",
    "                               random_state = 42, cv = 3)\n",
    "rf_model = RandomizedSearchCV(rf, rf_dict, 300, \n",
    "                               random_state = 42, cv = 3)\n",
    "log_model = GridSearchCV(log_reg, log_dict, cv = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Finds best parameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ada_params = ada_model.best_params_\n",
    "best_log_params = log_model.best_params_\n",
    "best_rf_params = rf_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Determines the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_log_reg = LogisticRegression(**best_log_params, multi_class = 'ovr', max_iter = 1000)\n",
    "best_ada = AdaBoostClassifier(**best_ada_params)\n",
    "best_rf = RandomForestClassifier(**best_rf_params, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.6,\n",
       "                   n_estimators=280, random_state=None)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.fit(x_train, y_train)\n",
    "best_log_reg.fit(x_train, y_train)\n",
    "best_ada.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = best_rf.predict(x_test)\n",
    "y_pred_ada = best_ada.predict(x_test)\n",
    "y_pred_log_reg = best_log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6352313167259785"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_rf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7307944845699278"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred_rf==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6215722120658136"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_ada, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7281680892974393"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred_ada==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6272401433691756"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_log_reg, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.726854891661195"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred_log_reg==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, clean_list, y, padding_start=True, N=40):\n",
    "        self.X1 = [encode_sentence(c_l, vocab2index, N=40, padding_start=False) for c_l in clean_list]\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x1,s = self.X1[idx]\n",
    "        return x1, s, self.y[idx]\n",
    "    \n",
    "train_ds = DisasterDataset(clean_list, y_train)\n",
    "valid_ds = DisasterDataset(clean_list, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 100\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=b_size)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, s, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 40])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(GRUModel, self).__init__() \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "    \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x1):\n",
    "        x = self.embeddings(x1)\n",
    "        x = self.dropout(x)\n",
    "        out, h = self.gru(x)\n",
    "        return self.linear(h[-1])\n",
    "    \n",
    "\n",
    "class LSTMModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super(LSTMModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        # sorting\n",
    "        s, sort_index = torch.sort(s, 0,descending=True)\n",
    "        s = s.numpy().tolist()\n",
    "        x = x[sort_index]\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pack = pack_padded_sequence(x, s, batch_first=True)\n",
    "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        return torch.zeros_like(out).scatter_(0, sort_index.unsqueeze(1).cuda(), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs_gru(model, optimizer, train_dl, val_dl, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x1, s, y in train_dl:\n",
    "            x1 = x1.long()#.cuda()\n",
    "            y = y.float()#.cuda()\n",
    "            y_pred = model(x1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_f1 = val_metrics_gru(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f val accuracy %.3f and val f1 %.3f\" % (sum_loss/total, val_loss, val_acc, val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics_gru(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    y_pred_all = []\n",
    "    y_all = []\n",
    "    for x1, s, y in train_dl:\n",
    "        x1 = x1.long()#.cuda()\n",
    "        y = y.float().unsqueeze(1)#.cuda()\n",
    "        y_hat = model(x1)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_pred = y_hat > 0\n",
    "        y_pred_all.extend(y_pred)\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        y_all.extend(y)\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total, f1_score(y_pred_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6495\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "gru_model = GRUModel(vocab_size, 50,50)#.cuda()\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, gru_model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.681 val loss 0.661 val accuracy 0.619 and val f1 0.348\n",
      "train loss 0.559 val loss 0.448 val accuracy 0.805 and val f1 0.747\n",
      "train loss 0.451 val loss 0.317 val accuracy 0.866 and val f1 0.837\n",
      "train loss 0.396 val loss 0.275 val accuracy 0.885 and val f1 0.860\n",
      "train loss 0.361 val loss 0.240 val accuracy 0.898 and val f1 0.881\n",
      "train loss 0.344 val loss 0.230 val accuracy 0.906 and val f1 0.891\n"
     ]
    }
   ],
   "source": [
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.311 val loss 0.213 val accuracy 0.911 and val f1 0.895\n",
      "train loss 0.307 val loss 0.208 val accuracy 0.915 and val f1 0.900\n",
      "train loss 0.300 val loss 0.201 val accuracy 0.915 and val f1 0.900\n",
      "train loss 0.293 val loss 0.197 val accuracy 0.915 and val f1 0.900\n",
      "train loss 0.300 val loss 0.193 val accuracy 0.919 and val f1 0.904\n",
      "train loss 0.287 val loss 0.190 val accuracy 0.918 and val f1 0.903\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.288 val loss 0.187 val accuracy 0.919 and val f1 0.904\n",
      "train loss 0.281 val loss 0.185 val accuracy 0.919 and val f1 0.905\n",
      "train loss 0.279 val loss 0.182 val accuracy 0.921 and val f1 0.906\n",
      "train loss 0.279 val loss 0.178 val accuracy 0.922 and val f1 0.909\n",
      "train loss 0.280 val loss 0.176 val accuracy 0.924 and val f1 0.910\n",
      "train loss 0.268 val loss 0.173 val accuracy 0.926 and val f1 0.913\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.268 val loss 0.171 val accuracy 0.926 and val f1 0.913\n",
      "train loss 0.265 val loss 0.168 val accuracy 0.926 and val f1 0.913\n",
      "train loss 0.272 val loss 0.167 val accuracy 0.926 and val f1 0.913\n",
      "train loss 0.262 val loss 0.165 val accuracy 0.928 and val f1 0.915\n",
      "train loss 0.257 val loss 0.164 val accuracy 0.927 and val f1 0.914\n",
      "train loss 0.259 val loss 0.163 val accuracy 0.928 and val f1 0.915\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.252 val loss 0.160 val accuracy 0.930 and val f1 0.917\n",
      "train loss 0.258 val loss 0.158 val accuracy 0.931 and val f1 0.918\n",
      "train loss 0.250 val loss 0.156 val accuracy 0.930 and val f1 0.917\n",
      "train loss 0.251 val loss 0.154 val accuracy 0.933 and val f1 0.921\n",
      "train loss 0.241 val loss 0.152 val accuracy 0.934 and val f1 0.922\n",
      "train loss 0.252 val loss 0.151 val accuracy 0.934 and val f1 0.921\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.240 val loss 0.150 val accuracy 0.933 and val f1 0.921\n",
      "train loss 0.242 val loss 0.148 val accuracy 0.934 and val f1 0.922\n",
      "train loss 0.247 val loss 0.147 val accuracy 0.935 and val f1 0.924\n",
      "train loss 0.237 val loss 0.147 val accuracy 0.935 and val f1 0.924\n",
      "train loss 0.242 val loss 0.144 val accuracy 0.936 and val f1 0.925\n",
      "train loss 0.234 val loss 0.141 val accuracy 0.938 and val f1 0.926\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.231 val loss 0.142 val accuracy 0.940 and val f1 0.929\n",
      "train loss 0.236 val loss 0.141 val accuracy 0.937 and val f1 0.926\n",
      "train loss 0.223 val loss 0.139 val accuracy 0.939 and val f1 0.928\n",
      "train loss 0.229 val loss 0.138 val accuracy 0.939 and val f1 0.928\n",
      "train loss 0.225 val loss 0.136 val accuracy 0.939 and val f1 0.928\n",
      "train loss 0.215 val loss 0.134 val accuracy 0.941 and val f1 0.930\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.220 val loss 0.133 val accuracy 0.941 and val f1 0.930\n",
      "train loss 0.216 val loss 0.132 val accuracy 0.940 and val f1 0.929\n",
      "train loss 0.230 val loss 0.131 val accuracy 0.941 and val f1 0.930\n",
      "train loss 0.220 val loss 0.130 val accuracy 0.941 and val f1 0.931\n",
      "train loss 0.221 val loss 0.130 val accuracy 0.940 and val f1 0.929\n",
      "train loss 0.217 val loss 0.129 val accuracy 0.942 and val f1 0.931\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.219 val loss 0.127 val accuracy 0.942 and val f1 0.931\n",
      "train loss 0.214 val loss 0.126 val accuracy 0.942 and val f1 0.931\n",
      "train loss 0.212 val loss 0.126 val accuracy 0.942 and val f1 0.932\n",
      "train loss 0.209 val loss 0.125 val accuracy 0.943 and val f1 0.933\n",
      "train loss 0.205 val loss 0.125 val accuracy 0.943 and val f1 0.932\n",
      "train loss 0.212 val loss 0.125 val accuracy 0.943 and val f1 0.932\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.206 val loss 0.123 val accuracy 0.944 and val f1 0.934\n",
      "train loss 0.213 val loss 0.122 val accuracy 0.943 and val f1 0.933\n",
      "train loss 0.194 val loss 0.122 val accuracy 0.943 and val f1 0.933\n",
      "train loss 0.198 val loss 0.121 val accuracy 0.943 and val f1 0.932\n",
      "train loss 0.202 val loss 0.120 val accuracy 0.944 and val f1 0.934\n",
      "train loss 0.203 val loss 0.119 val accuracy 0.944 and val f1 0.934\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.214 val loss 0.120 val accuracy 0.944 and val f1 0.934\n",
      "train loss 0.202 val loss 0.118 val accuracy 0.944 and val f1 0.934\n",
      "train loss 0.196 val loss 0.119 val accuracy 0.945 and val f1 0.935\n",
      "train loss 0.202 val loss 0.117 val accuracy 0.944 and val f1 0.935\n",
      "train loss 0.192 val loss 0.117 val accuracy 0.945 and val f1 0.935\n",
      "train loss 0.186 val loss 0.116 val accuracy 0.946 and val f1 0.936\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.197 val loss 0.115 val accuracy 0.946 and val f1 0.937\n",
      "train loss 0.203 val loss 0.114 val accuracy 0.946 and val f1 0.937\n",
      "train loss 0.188 val loss 0.114 val accuracy 0.946 and val f1 0.937\n",
      "train loss 0.195 val loss 0.114 val accuracy 0.946 and val f1 0.937\n",
      "train loss 0.190 val loss 0.113 val accuracy 0.946 and val f1 0.936\n",
      "train loss 0.191 val loss 0.113 val accuracy 0.946 and val f1 0.936\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.187 val loss 0.113 val accuracy 0.946 and val f1 0.936\n",
      "train loss 0.189 val loss 0.112 val accuracy 0.947 and val f1 0.937\n",
      "train loss 0.187 val loss 0.111 val accuracy 0.946 and val f1 0.937\n",
      "train loss 0.183 val loss 0.110 val accuracy 0.947 and val f1 0.937\n",
      "train loss 0.183 val loss 0.110 val accuracy 0.947 and val f1 0.937\n",
      "train loss 0.185 val loss 0.110 val accuracy 0.946 and val f1 0.938\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.183 val loss 0.109 val accuracy 0.948 and val f1 0.939\n",
      "train loss 0.178 val loss 0.109 val accuracy 0.948 and val f1 0.939\n",
      "train loss 0.173 val loss 0.109 val accuracy 0.947 and val f1 0.939\n",
      "train loss 0.180 val loss 0.108 val accuracy 0.948 and val f1 0.939\n",
      "train loss 0.185 val loss 0.108 val accuracy 0.947 and val f1 0.938\n",
      "train loss 0.170 val loss 0.108 val accuracy 0.949 and val f1 0.939\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.182 val loss 0.107 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.177 val loss 0.107 val accuracy 0.948 and val f1 0.939\n",
      "train loss 0.166 val loss 0.106 val accuracy 0.948 and val f1 0.939\n",
      "train loss 0.172 val loss 0.106 val accuracy 0.948 and val f1 0.940\n",
      "train loss 0.174 val loss 0.106 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.171 val loss 0.105 val accuracy 0.948 and val f1 0.940\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.172 val loss 0.104 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.170 val loss 0.104 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.173 val loss 0.104 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.170 val loss 0.104 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.158 val loss 0.104 val accuracy 0.948 and val f1 0.940\n",
      "train loss 0.165 val loss 0.104 val accuracy 0.949 and val f1 0.939\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.165 val loss 0.103 val accuracy 0.948 and val f1 0.940\n",
      "train loss 0.171 val loss 0.103 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.160 val loss 0.104 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.161 val loss 0.102 val accuracy 0.950 and val f1 0.941\n",
      "train loss 0.162 val loss 0.103 val accuracy 0.949 and val f1 0.941\n",
      "train loss 0.166 val loss 0.103 val accuracy 0.949 and val f1 0.940\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.166 val loss 0.102 val accuracy 0.950 and val f1 0.941\n",
      "train loss 0.161 val loss 0.101 val accuracy 0.950 and val f1 0.941\n",
      "train loss 0.161 val loss 0.101 val accuracy 0.949 and val f1 0.940\n",
      "train loss 0.168 val loss 0.100 val accuracy 0.950 and val f1 0.941\n",
      "train loss 0.156 val loss 0.100 val accuracy 0.950 and val f1 0.942\n",
      "train loss 0.159 val loss 0.100 val accuracy 0.950 and val f1 0.942\n"
     ]
    }
   ],
   "source": [
    "update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs_gru(gru_model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
